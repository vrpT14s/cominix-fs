Thu Feb 13 09:04:29 PM IST 2025
This is a filesystem for linux that does deduplication with content-defined chunking (CDC) as described in the FastCDC paper (Wen Xia et al., 2016)[1]. It breaks a file into chunks and puts these chunks in a hashtable, then stores a list of those hashes[i]. This way, if two files have a chunk in common, it's only stored once on disk. One way to break a file into chunks is just to break it into fixed size blocks one after another. This is called fixed size chunking (FSC). If you only replace bytes then this works nicely but if you add a byte in the middle of a file, all the bytes in every block will be shifted to the right, so they'll be considered new blocks since they have new hashes, so this means you have to use more storage. To get around this we make the boundaries depend on the data, so local features, that won't be bothered by inserting and deleting bytes in the middle. 

#A description of CDC
If we were dealing with only English sentences then we could define our boundary to be fullstops. So our hashtable becomes a hashtable of sentences. If we change a sentence then we only need to store the new sentence, we don't need to store other parts of the file again. This would work perfectly if you knew your data had that kind of format, that kind of structure. Otherwise you'd be troubled by the edges cases of having too many fullstops or not having any fullstops at all. If we tried to use any kind of delimeter we'd run into the same problem. So instead of looking at the data, we look at the hash. We hash the last 64 bytes of data and then check whether it's some special value. In general we need to look for some kind of local feature. We need some kind of ```is_boundary``` function that takes in local data and returns true or false, such that true is only returned every 8 kilobytes or so, so our chunks aren't too large or too small. Maybe you can guess that we can use a hash the data around a point, in order to remove all structure and make the distribution of outputs uniform, and then check some property of the hash such that that property only comes up in some 64 bit numbers, say if it's divisible by floor(1/p). The issue here is that it's expensive to compute so many hashes. So we use a "rolling hash" which let's us shift data in and out of the hash quickly. I won't explain it here but you can read it elsewhere, for example CLRS talks about how it can be used in order to search for words in a text quickly. This is named "Rabin-Karp search" (after the inventors), and the rolling hash they use is called the Rabin hash. It isn't important how it's implemented, you just need to think of it as a shortcut for when you want to hash a certain window of data and this window shifts (i.e. rolls) by a byte or a few bytes each time. Here we use 64 byte windows. Thus we've made a machine that moves (rolls) through a file and roughly every 8kb or so, it lights up and tells us to make a chunk boundary there. This is what CDC is. Usually we check if certain bits of the hash are unset instead of dividing it by a number though, simply because it's faster[ii]. You could check divisibility and it would still work almost at the same speed. 

I don't really know about the history of CDC deduplication but I did see a paper from Usenix 2008 about a CDC-deduplicating filesystem (which I can't find now), so it's at least as old as 2008. I found out about CDC from reading an article about a recent CDC filesystem called PuzzleFS on linux weekly news. I'll write more about why I chose this topic later. In 2016 there was a paper that I think was quite popular called FastCDC. The problem with using the Rabin rolling hash is that it's pretty slow, so people developed something called the gear rolling hash which was faster but often made your chunks too small or too large still, so it gave bad deduplication. The FastCDC paper improved it by changing the property that we're looking at (i.e. changing how many bits we're checking are set) when the current chunk size goes past a certain threshold. This gives a better distribution of chunk sizes. It also adds a minimum for the chunk size (which can improve speed because it let's us skip looking at some areas) and changed which bits of the hash they were checking because of some peculiarities of gear hashing (specifically the rabin hash mixes information somewhat equally into all bits but the gear hash stores later information in the higher bits and newer information in both higher and lower bits). That's all you need to know about CDC to understand this project.

#A short practical description of CDC deduplication
If you take two files that are pretty similar, then the storage taken should only be the size of a single file plus roughly 8kb times the number of different places you've added or removed information. It's 8kb becaues we've set that to be our (roughly) average chunk size. You can change it by changing some constants.

#Why I was interested in this program
I'm not particularly interested in data or data efficiency. The first reason I wanted to make a filesystem because it can be separately developed from an operating system, they're modular. There are 4 parts to any OS: I/O (device drivers etc.), FS (filesystems), PC (process control) and MM (memory management). Device drivers are the most separate but I have no interest in hardware. For process control you could make a new form of IPC but that's a bit niche. With MM I don't know anything that you could change in a modular manner. You could change a scheduling algorithm somewhere but that's too deep and too entrenched in the OS. The second reason is that the first thing you hear about UNIX is that everything is a file. Really this means everything is a file descriptor, so it can be opened, closed, read and written to. So I was interested in understanding the design there. I still don't properly understand it because in order to do that I'd need to look at a non-unix operating system.

#Filesystems on UNIX
One aspect of files is that they're a mapping between byte offsets (the 100th byte in a file) with bytes on storage (the 50th byte in the 90th block on some hard drive). Usually we don't deal with bytes of course, we deal with whole blocks, which are just 512 or 4kb bytes together (You can change the size but it has to be some multiple of 512). Thus we have the question of how to store the map from logical blocks to storage blocks. In the original unix file system (called UFS), we basically stored it in an array. Since large files wouldn't fit we would make it a general tree, whose structure doesn't change, by making an array of pointers to other arrays (this is called a single indirect block) and then an array to pointers to pointers (double indirect) and even triple indirect ones. This is perfectly efficient and I can't see how you'd make it better. The issue here is that today you don't want to talk to a file in terms of single blocks. You want to directly deal with much larger areas, at least megabyte sized. We call these extents, and they're just contiguous sequences of storage blocks. The UFS tree (i.e. an array) fails here. Usually most filesystems today use a B-tree (zfs, ext4, BTRFS, xfs).

The second aspect of filesystems is directories. This is a mapping between file/folder names and the metadata of the file/folders. Since it's troublesome to say "file/folders" and we can also have things like named devices or named pipes, we call this name in general a "directory entry" or a dentry in short. The metadata is always stored in what's called in UNIX terminology an inode. It stores the filesize, the type of the file (whether it's a folder or a temporary pipe or a named pipe etc.) and then a pointer to the logical block/storage block mapping for standard files. In unices it doesn't store the name of the file. So here we need a mapping between dentries (i.e. names) and inodes (i.e. file metadata). One way to do this is to treat the directory as a list of filenames. This is how it was done in UFS. The problem is that you'll need to sweep through the whole list to find your entry. If you put a filename maximum, then you can treat it as an array instead. I think UFS did this but I'm not sure. The minix filesystem (which I'll talk about later) does do it this way. Modern filesystems usually also use a B-tree here. My program doesn't change this at all so I won't talk about it.

Those two are the first two choices you have when making an operating system. One other piece you need to deal with is metadata about the filesystem as a whole. For example which inode is the root of a filesystem. We store this global metadata in a block called the "superblock".

A third choice might be how you allocate and free blocks and inodes. UFS and minix-fs just use a bitmap. I don't change this either.

#My program and its relation to MINIX
As you may know, linux was based on the design of a free operating system called MINIX. MINIX was developed by a professor called Andrew Tanenbaum with the goal of making an operating system that students can actually understand and make changes to. He published a textbook about it along with the source code of MINIX at the back. Because of this, the first filesystem on linux was just the filesystem on MINIX, which was unsusprisingly called the MINIX filesystem, or minix fs. Because of some of its drawbacks, maybe most visibly being its small filename length limit, Linus et al. made some improvements and made ext2, where ext means extended. Apparently they tried to make "ext" first but that had some problems so they only published the second version. It still used the UFS block table but changed the dentry mapping in some ways (I don't know the specifics). 

I'll give a short history of notable filesystems after that (this isn't really related to this project though). The largest issue with ext2 was that if your computer turned off without warning, the filesystem's structure might become invalid (i.e. corruption). To deal with this came logging and journalling filesystems, where filesystems describe what they're about to do before they do it so even if you lose some data, you can still save the structure and stop the filesystem from getting corrupted. I think JFS (the journalling filesystem) was the first on linux but ReiserFS was one of the first and pretty popular too. Eventually they made ext3, which was adding journalling and was backwards compatible with ext2. It also used a B-tree for the dentry mapping but that isn't important to us.

-- I'll finish this description later

#The structure of my program
The disk is split into two, the first part is for directories and temporary files. The second part is where the chunks are stored and at the start, the hashtable for the chunks. The default here is that the hashtable is 32 kb long and is at 40 mb. You can change both but it can't grow dynamically. When a file is made, it's just a normal minix file and is in the first part. We chunk it and move it to the second part by sending the path of the file to a proc entry (proc entries are just a loose kind of way to transmit information to a module). Then in the first part we only have a linked list containing chunk locations and nodes.

You can only add chunks. You can't remove them. The reason is that it simplifies the processing. If you were able to remove chunks then I'd need to implement a general malloc-style hole handling algorithm. That's too difficult. It's not a matter of time. It's that the project would collapse in on itself from complexity, because I'm not experienced enough to handle it. Thus chunked files are read-only.

I also add an "extra superblock" which is really just a hack to avoid changing the superblock structure of minix so I don't need to make another mkfs program. Minix3-fs has 48 pad bits in the superblock so I just cram a block address there and store my extra information, like how large the chunk stack is, over there. Another hack is that in order to not change the inode structure, when I want to signal that it's a chunked file, I put the superblock sector (which is sector number 1) as the mapping for the first logical block (and the 9th too), since I know no normal file should have that address.

I think that's it about the description.

#File-level source code explanation
For the minix-fs module on linux:
inode.c - for handling inode formats and allocating them etc., as well as for dealing with superblocks and mounting
bitmap.c - for block and inode allocation
itree_*.c - by "itree" they mean the UFS tree I was talking about. This handles that structure.
namei.c, dir.c - dentry and directory handling
file.c - mostly glue code

The files I've added:
chunk_handler.c - handling the stack of chunks (i.e. the heap) and the hashtable
gear_table.c - just an array of fixed random numbers that I use for gear hashing
linked_list.h - handling the linked list of chunk hashes for each file
md5_wrapper.h - handles hashing (not gear-hashing!) buffers (it's a wrapper for md5 in the linux crypto API)
cdc.h - implements FastCDC (basically a complete copy of the pseudocode in the paper)
file.c - Acts as glue code like before but also chunking files and reading chunked files happen here. I also set up the proc entry here when really it should be in a different file...


I also have to change inode.c in some places to handle the extra superblock.

#Outstanding bugs/issues, and about complexity
Design Issues:
- The largest issue is that chunked files are read-only, like I said. To fix this I'd need to make a malloc-like algorithm.
- The second largest issue is how I store my list of chunk hashes. I should change this to a B-tree.
Programming Issues:
- To read and write blocks, I use buffer heads because they're the simplest to use. They're inefficient though. I should upgrade to using bio's and maybe even iomap, but I don't know how easy that is to to use.
- I sacrifice efficiency for programming simplicity in lots of places. For example, when I hash, I malloc and free some small buffer that I really can just keep fixed. I didn't because it would make it more complicated. There's definitely some unneeded copying of large file buffers in some other places.


[i] - Actually I store the locations of chunks directly because it was easier. It's maybe a 4 line change to store hashes instead though.
[ii] - If you only check if the lower bits are unset, then that's just the same as checking if it's divisible by a certain power of two.

[1] - https://www.usenix.org/system/files/conference/atc16/atc16-paper-xia.pdf

